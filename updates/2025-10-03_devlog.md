## 2025-10-03 — Streaming Chat Refactor (Convex httpAction + AI SDK useChat)

Summary
- Implemented true token-by-token streaming via Convex httpAction and Vercel AI SDK useChat.
- Added new backend endpoint and refactored frontend chat context; dev proxy to avoid CORS.
- Fixed TS types for provider init and onFinish handling per AI SDK v5.

Changes
- Backend:
  - Added: ea-ai-main2/ea-ai-main2/convex/ai/stream.ts (httpAction using streamText; persists onFinish).
  - Updated: ea-ai-main2/ea-ai-main2/convex/http.ts (register POST /chat -> stream action).
- Frontend:
  - Updated: ea-ai-main2/ea-ai-main2/src/context/chat.tsx (switch to @ai-sdk/react useChat; Clerk auth header; normalize messages).
  - Updated: ea-ai-main2/ea-ai-main2/vite.config.ts (proxy /convex-http/* to Convex dev).

Notes
- onFinish must be passed to streamText (finish.response.messages), not to toUIMessageStreamResponse.
- Google Vertex provider used via providerClient.any().chat(modelName) pattern mirroring session.ts.
- Persisted assistant message uses messageSchemas.createEmbeddedMessage.

Next
- Validate tool call/result embedding in persisted history.
- Expand error telemetry and usage metrics.

---

Runtime Fixes — Convex runtimes alignment (Node vs default JS)

Summary
- Investigated bundling errors (Could not resolve net/http/tls) caused by Node-only deps used from Convex isolate runtime.
- Per Convex docs, functions using Node APIs must opt into Node runtime with "use node" at file top.

Changes
- Keep Node runtime only where required:
  - Added "use node" to: convex/ai/compaction.ts, convex/ai/subagents/executor.ts.
- HTTP streaming endpoint must run in Convex default JS runtime:
  - Removed "use node" from convex/ai/stream.ts; removed Vertex import; restricted streaming endpoint to OpenRouter only (fetch-based). If provider === google, returns 400 with guidance.
  - convex/http.ts cannot use Node runtime; switched `@clerk/backend` to type-only import to avoid bundling Node deps.

Status
- Bundling errors for Node built-ins from httpAction are resolved by isolating Node-only deps to Node actions and keeping httpAction Node-free.
- Frontend follow-up: saw "Cannot read properties of undefined (reading 'trim')" during UI run; fixed by normalizing input and guarding trim calls.

---

Frontend Hotfix — trim() crash after streaming refactor

Summary
- After switching to @ai-sdk/react useChat + Convex http streaming, the chat UI crashed because `input` could be undefined initially and multiple locations called `trim()` directly.

Fix
- src/context/chat.tsx: expose `input ?? ""` via context to guarantee string.
- src/components/chat/ChatInput.tsx: guard submit button disabled check with optional chaining `(value?.trim?.().length ?? 0) === 0`.
- src/components/chat/Chat.tsx: guard Enter handler using `(input ?? "").trim().length === 0`.

Result
- UI no longer throws on first render; streaming and submit interactions work as expected.

References
- Convex docs: Functions → Actions → Node.js runtime ("use node").
- Vercel AI SDK streaming examples assume Node server backends.

---

Frontend Hotfix — input freeze + diagnostics (evening)

Summary
- Chat textarea showed caret but keystrokes didn’t render. Root cause was SDK API mismatch: current @ai-sdk/react useChat did not expose input/handleInputChange/setInput in our setup; our onChange crashed.

Fix
- src/context/chat.tsx:
  - Memoized DefaultChatTransport and injected Authorization via prepareSendMessagesRequest headers.
  - Replaced SDK-managed input with local state (localInput), added logging for status/input lifecycle.
  - Exposed handleInputChange to update localInput and handleSubmitCompat that calls sendMessage({ text }).
- src/components/chat/Chat.tsx and ChatInput.tsx:
  - Added lightweight UI logs (onChange, onKeyDown, focus, disabled/valueLen) to trace interactions.

Result
- Typing now updates the field reliably; submit triggers sendMessage. Post-submit we see a partial error in dev server: "[vite] http proxy error: /chat ECONNREFUSED" (Convex HTTP endpoint not reachable via proxy).

Next
- Investigate Vite proxy to /convex-http/chat and Convex dev server availability; ensure target URL/port matches running Convex, or fallback to full origin.
  - Repro snippet: AggregateError ECONNREFUSED after send.
  - Verify Clerk auth header present on request.

---

Evening Debugging — proxy alignment and streaming 500

Summary
- Switched to a Vite dev proxy pattern: frontend calls `/convex-http/chat` which Vite rewrites to Convex `/chat` to keep same‑origin for SSE and attach auth headers.
- Updated vite.config.ts to load env and resolve proxy target as `VITE_CONVEX_HTTP_ORIGIN || VITE_CONVEX_URL || http://localhost:3210`. Added `.env.development.local` with `VITE_CONVEX_HTTP_ORIGIN=https://peaceful-boar-923.convex.cloud`.
- Observed progression of errors when sending a chat message:
  - First: ECONNREFUSED to `/chat` (proxy defaulted to 3210 with no local Convex).
  - Then: 404 on `/convex-http/chat` and `/telemetry/oauth-callback` hitting cloud that didn’t have the nested app’s http routes deployed.
  - Now: 500 on `/convex-http/chat` (Internal Server Error) — Convex route is reached but handler fails inside.

What’s different vs main
- Main branch uses Convex client actions (useAction(api.ai.session.chatWithAI)) — no proxy, no httpAction streaming.
- Current branch uses AI SDK `useChat` + Convex `httpAction` `/chat` with `streamText(...).toUIMessageStreamResponse()`.

Likely root cause of current 500
- The streaming httpAction (`convex/ai/stream.ts`) requires a globally selected model and valid provider credentials in the Convex deployment env.
- Code path returns:
  - 500 when no `activeModelId` is set ("No model selected...").
  - 400 if provider is `google` (Vertex not supported in this httpAction runtime).
- Admin logs show model toggled to Google Gemini (google/...), and OpenRouter details are fetched separately. If Google is selected globally, the httpAction will reject; if no OpenRouter key is set in the Convex cloud env, it can also fail.

Evidence (logs)
- Browser: repeated `:5173/convex-http/chat` → 500 after session creation; earlier 404s cleared, implying correct routing now.
- Admin dashboard logs show model selection events (x-ai/grok-4-fast, then google/gemini-2.5-...); streaming endpoint only supports OpenRouter (per earlier runtime notes).

Status
- Proxy and routing are aligned to cloud; the http route is invoked.
- Streaming still fails with 500 due to provider/model/env mismatch on the Convex deployment.

Next (not executed — planning only)
- Ensure nested Convex app is deployed to `peaceful-boar-923` so all http routes exist.
- In Convex dashboard or via CLI, set `OPENROUTER_API_KEY` on the dev deployment.
- In Admin UI, set the global active model to an OpenRouter model (not Google) for the streaming endpoint.
- Re‑test `/convex-http/chat`; if it still fails, capture response body from Network tab to confirm server error message.

---

Late‑evening note — Endpoint verification and migration option

Summary
- Verified that `/chat` is intentionally a POST httpAction (convex/http.ts) to support Vercel AI SDK streaming; frontend calls `/convex-http/chat` which Vite rewrites to `${VITE_CONVEX_HTTP_ORIGIN}/chat`.
- Default to `http://localhost:3210` comes from vite.config.ts when `VITE_CONVEX_HTTP_ORIGIN` is unset (Convex dev server default port). Not a Convex doc default, ours.
- 404s for `/telemetry/oauth-callback` occur because the client issues GET while backend defines POST only.

Findings
- 500 on `/chat` is almost certainly due to using `apiProvider=google` or missing `OPENROUTER_API_KEY` while stream.ts only supports OpenRouter in the httpAction runtime.
- Correct way to “not guess” endpoints: Convex Dashboard → HTTP Routes → copy base URL for `/chat`; also verify Network tab shows rewritten Request URL.

Planned actions
- Align provider: set apiProvider to `openrouter`, select an OpenRouter model globally (activeModelId), and set `OPENROUTER_API_KEY` in Convex env.
- Quiet telemetry noise by switching client to POST for `/telemetry/oauth-callback` or adding a GET handler.
- Consider migrating to Convex Agent component (@convex-dev/agent) to avoid maintaining a custom HTTP streaming path; use actions + streaming queries (useUIMessages/useSmoothText) for built‑in streaming.

---

Morning Follow-up — Simplified streaming stub & outstanding issues (Oct 4)

Summary
- Refactored `convex/ai/stream.ts` to be a minimal httpAction: authenticate, pick OpenRouter model, forward UI messages via `streamText`, then persist history in `onFinish`.
- Frontend transport now derives the Convex site URL (`VITE_CONVEX_HTTP_ORIGIN` → `/chat`) instead of hardcoding `/convex-http/chat`; added fallback to window origin for local dev.
- Vite dev server exposes both `/chat` and legacy `/convex-http` proxies to ease migration.

Status / Observations
- Browser console still reports 404s for `https://peaceful-boar-923.convex.cloud/chat`; cloud deployment does not yet have the new httpAction published.
- `/telemetry/oauth-callback` GETs still 404 because backend only defines POST — unchanged.
- Local dev succeeds in writing history when the endpoint returns 200, but cloud requests fail before hitting persistence due to missing route.

Next
- Deploy updated Convex functions to the `peaceful-boar-923` instance and verify `/chat` exists.
- After deploy, re-test streaming flow; if response remains 500, capture server logs for model/key diagnostics.

---

CORS + Auth — /chat on convex.site and Clerk JWT (Oct 3 late)

Summary
- Frontend now targets Convex HTTP actions on `.convex.site` for both `/chat` and telemetry; removed `.cloud` usage for HTTP actions.
- Added CORS to all HTTP action responses (POST + early 4xx/5xx) and explicit OPTIONS handlers for `/chat` and telemetry; echoes Origin and allows `Authorization`.
- Updated client to send a Convex‑verifiable Clerk token via `getToken({ template: 'convex' })` in the `Authorization: Bearer` header.

Status
- CORS preflight now succeeds locally, but cloud POST `/chat` returns 401 because `ctx.auth.getUserIdentity()` is null (token not recognized by Convex env).

Actions Required
- Configure Convex env with `CLERK_JWT_ISSUER_DOMAIN` matching the Clerk "convex" JWT template issuer; ensure Clerk has a JWT template named `convex`.
- Keep `CLIENT_ORIGIN=http://localhost:5173` in Convex env for dev; retry POST `/chat` and confirm 200/stream.

