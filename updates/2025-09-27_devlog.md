# 2025-09-27 Development Log

## Session Start
- **Date**: September 27, 2025
- **Time**: 6:48 PM
- **Status**: Starting startup routine

## Tasks Completed

### Major Prompt Improvements
1. **Fixed Planning Mode Conflicting Prompts**
   - **Problem**: Planning mode had conflicting instructions - `planning_new.ts` demanded "UNDER 150 CHARACTERS" but `registry.ts` asked for "systematic plans with clear next steps"
   - **Root Cause**: LLM was receiving both prompts and prioritizing the later, more verbose instruction
   - **Fix**: Updated registry.ts planning mode promptInjection to enforce brevity while keeping conversational tone
   - **Files Modified**: `convex/ai/modes/registry.ts`, `convex/ai/prompts/planning_new.ts`

2. **Information Collector Complete Rewrite**
   - **Problem**: Robotic, formal language using "QUESTION_FOR_USER:" markers and awkward phrasing
   - **Fix**: Completely rewrote to natural conversation style
   - **Reduced from**: 161 lines to 79 lines (well under 100-line requirement)
   - **Language Changes**:
     - Removed: "What's your deadline for...", "Do you have a deadline in mind?"
     - Added: "When does this need to be done?", "How much effort would that take?"
   - **Data Collection**: Reduced from 3 points (deadline, effort, dependencies) to 2 points (timing, effort only)
   - **File Modified**: `convex/ai/prompts/information_collector_new.ts`

3. **Removed Forced/Artificial Language**
   - **Eliminated**: "Let's start with...", "To prioritize...", "Sounds overwhelming!"
   - **Added explicit rules**: Clear "NEVER USE" list for LLM to follow
   - **Result**: More natural, direct questions without meta-commentary

## Technical Issues Encountered

### Grok-4-Fast Rate Limiting Error
- **Time**: 19:30:54
- **Error**: `AI_RetryError` - "Provider returned error" (Status 429)
- **Root Cause**: `x-ai/grok-4-fast is temporarily rate-limited upstream`
- **Details**: Failed after 4 retry attempts through OpenRouter
- **Impact**: User request "I've got a bunch of stuff swirling around in my head..." failed to process
- **Error Response**: "No output generated. Check the stream for errors."
- **Note**: This is an upstream provider issue, not a code problem

## Files Modified
- `convex/ai/modes/registry.ts` - Fixed planning mode promptInjection
- `convex/ai/prompts/planning_new.ts` - Updated for conversational brevity  
- `convex/ai/prompts/information_collector_new.ts` - Complete rewrite (161→79 lines)

## Key Insights
- **Conflicting prompts** cause verbose output even when one prompt demands brevity
- **Mode registry promptInjection** takes precedence over main prompt files
- **Natural language examples** work better than formal business terminology
- **Line count reduction** forces essential-only content, improving clarity

## Enhanced Error Handling Implementation (Evening Session)

### OpenCode Analysis & Rate Limit Resilience
1. **Comprehensive Investigation**
   - **Analyzed**: OpenCode's message handling, session management, and retry strategies
   - **Key Findings**: OpenCode uses maxRetries: 3-10, structured error types, exponential backoff
   - **Digest Evaluation**: Partially accurate but misleading - rate limits are upstream provider issues, not message structure problems

2. **Enhanced Error Handling System**
   - **Added Error Types**: `RateLimitError`, `ProviderAuthError`, `ModelNotFoundError`
   - **Retry Strategy**: 5 attempts with exponential backoff (1s→2s→4s→8s→16s)
   - **Smart Detection**: Multiple rate limit patterns including "temporarily rate-limited upstream"
   - **Retry-After Headers**: Parses and respects provider timing guidance
   - **Jitter Added**: Prevents thundering herd with random 0-1s variance

3. **User Experience Improvements**
   - **Clear Messages**: Rate limits explain wait times and that it's normal during peak usage
   - **No Model Fallbacks**: Maintains user-selected model from admin dashboard
   - **Enhanced Logging**: Detailed retry attempts and timing information
   - **Metadata Enhanced**: Added errorType and retryAfter properties

4. **Implementation Details**
   - **File Modified**: `convex/ai/session.ts`
   - **maxRetries**: Increased from 3 to 8 in AI SDK calls
   - **Wrapper Function**: `streamTextWithRateLimit()` with custom retry logic
   - **Type Safety**: Updated `ChatMetadata` interface for new properties

### Build Issues Encountered
- **Error**: Convex bundler cannot resolve Node.js built-ins (net, tls, assert, url, http, https)
- **Root Cause**: Missing "use node" directive for Node.js runtime
- **Affected Files**: Likely OpenRouter/agent dependencies
- **Status**: Documented but not resolved - requires environment fixes

## Files Modified Today
- `convex/ai/modes/registry.ts` - Fixed planning mode promptInjection
- `convex/ai/prompts/planning_new.ts` - Updated for conversational brevity  
- `convex/ai/prompts/information_collector_new.ts` - Complete rewrite (161→79 lines)
- `convex/ai/session.ts` - **MAJOR**: Enhanced error handling with retry resilience
- `convex/ai/test-error-handling.js` - **NEW**: Test suite for error classification

## Key Insights
- **Conflicting prompts** cause verbose output even when one prompt demands brevity
- **Mode registry promptInjection** takes precedence over main prompt files
- **Natural language examples** work better than formal business terminology
- **Rate limiting** is primarily an upstream provider capacity issue, not code architecture
- **OpenCode's retry strategy** can be adapted without adopting complex message structures
- **Exponential backoff** with jitter prevents cascade failures during rate limits

## Next Steps
- **Immediate**: Resolve Convex Node.js bundling issues with "use node" directives
- **Testing**: Deploy enhanced error handling to test rate limit resilience
- **Monitoring**: Track retry success rates and timing effectiveness
- **Optimization**: Adjust retry parameters based on real-world provider behavior