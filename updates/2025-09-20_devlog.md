# Devlog - September 20, 2025

## Development Session Log

## Agent Prompt Unification - Behavioral Consistency Fix

**Date**: September 20, 2025 - 12:15 PM - Prompt Engineering Session
**Status**: ✅ Tested and working

Identified critical behavioral inconsistencies in 4-mode agent system where agents were revealing internal architecture and providing verbose responses instead of maintaining unified Zen persona.

**Problem Analysis:**
User tested system with overwhelmed scenario and agents produced walls of text, multiple questions, and references to separate "information-collector agents" instead of seamless Zen experience. Root cause was prompts still allowing verbose explanatory behavior and agent self-reference.

**Engineering Decision Process:**
Made the call to completely eliminate agent self-awareness in prompts. Analyzed three approaches: 1) Incremental prompt tweaking, 2) Complete prompt rewrite, 3) Hybrid approach with strict character limits. Chose hybrid approach because it preserved working delegation logic while enforcing behavioral constraints.

**Implementation Changes:**
- **Primary Agent (`zen_new.ts`)**: Removed all references to "specialists" and "delegation" - now speaks only of using "internal tools"
- **Information Collector (`system.ts`)**: Reduced character limit from 50→30 chars, added specific examples of recent bad verbose behavior, eliminated all reassurance/explanation language
- **Planning (`planning_new.ts`)**: Removed "ANALYSIS_COMPLETE:" format, now speaks directly to user as Zen
- **Execution (`execution_new.ts`)**: Removed "EXECUTION_COMPLETE:" format, direct confirmations only

**Key Insight - Persistent Behavior Problem:**
The breakthrough came when realizing prompts needed to address behavior consistency across ALL interactions, not just first response. Added explicit "APPLIES TO ALL INTERACTIONS" language to prevent model regression to verbose patterns.

**Current Status:**
All prompts updated with ultra-strict behavioral constraints. Information-collector now operates with robotic brevity (under 30 characters, zero explanations). System maintains unified Zen identity across all modes.

**Expected Behavior:**
User: "I'm drowning with tasks..." → Zen: "Let me help you organize this." → "What are your work deadlines?"

**References:**
- Modified: `convex/ai/prompts/zen_new.ts:1-59`
- Modified: `convex/ai/prompts/system.ts:141-202` 
- Modified: `convex/ai/prompts/planning_new.ts:1-63`
- Modified: `convex/ai/prompts/execution_new.ts:1-70`

## Information Collection Data Points Specification - Persistent Issues

**Date**: September 20, 2025 - 1:30 PM - Emergency Debugging Session
**Status**: ❌ Attempted but failing

Attempted to resolve persistent therapeutic/coaching behavior in information-collector agent despite multiple prompt revisions. Agent continues providing emotional support and asking multiple questions instead of collecting specific data points.

**Final Problem State:**
Even after complete prompt rewrite with "DATA COLLECTOR" role definition and explicit forbidden behaviors list, agent still responds with:
- "I want to approach this gently..."
- "It sounds like you're carrying a lot of mental weight..."
- "The information-collector agent will help us..."
- Multiple questions in single response
- Reassurance and emotional validation

**Engineering Analysis:**
The core issue appears to be deeper than prompt engineering can resolve. Despite defining agent as pure "DATA COLLECTOR" with 25-character limits and explicit anti-therapy language, the model defaults to helpful assistant patterns. Three successive iterations failed to prevent regression to coaching behavior.

**Attempted Solutions:**
1. **Ultra-strict character limits** (50→30→25 characters)
2. **Role redefinition** (Assistant→Data Collector→Function)
3. **Massive forbidden behaviors list** (specific phrases from bad examples)
4. **3-data-point focus** (deadline, time/work, dependencies)

**Current Status:**
Agent behavior remains inconsistent with requirements. The information-collector continues providing therapeutic support instead of asking single questions for data collection. This suggests the issue may be at the model instruction level or require architectural changes beyond prompt engineering.

**Lessons Learned:**
Sometimes prompts cannot override fundamental model behavior patterns. The "helpful assistant" training may be too strong to suppress through prompt engineering alone. May require different architectural approach or model fine-tuning.

**References:**
- Failed: `convex/ai/prompts/system.ts:141-198` (Information collector rewrite)
- Issue: Model ignoring explicit role constraints and character limits
